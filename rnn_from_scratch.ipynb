{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 585 words, 332 unique.\n"
     ]
    }
   ],
   "source": [
    "data = open('sentences.txt', 'r').read()\n",
    "words = tokenize(data)\n",
    "word_counts = Counter(words)\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "data_size, vocab_size = len(words), len(vocab)\n",
    "print('data has %d words, %d unique.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {word: i for i, word in enumerate(vocab)}\n",
    "ix_to_word = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 \n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "\n",
    "Bh = np.zeros((hidden_size, 1))\n",
    "By = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN Model Making**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(inputs, targets, hprev):\n",
    "    xs, hs, ys, ps= {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    #forward propagation\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + Bh)\n",
    "        ys[t] = np.dot(Why, hs[t]) + By\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))\n",
    "        loss += -np.log(ps[t][targets[t], 0])\n",
    "\n",
    "    #back propagation\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dBh, dBy = np.zeros_like(Bh), np.zeros_like(By)\n",
    "    dHnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dBy += dy\n",
    "        dh = np.dot(Why.T, dy) + dHnext\n",
    "        dHraw = (1 - hs[t] * hs[t]) * dh\n",
    "        dBh += dHraw\n",
    "        dWxh += np.dot(dHraw, xs[t].T)\n",
    "        dWhh += np.dot(dHraw, hs[t-1].T)\n",
    "        dHnext = np.dot(Whh.T, dHraw)\n",
    "\n",
    "    for dparam in [dWxh, dWhh, dWhy, dBh, dBy]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)\n",
    "        \n",
    "    return loss, dWxh, dWhh, dWhy, dBh, dBy, hs[len(inputs)-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 10000\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mBh, mBy = np.zeros_like(Bh), np.zeros_like(By)\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next(text, hprev):\n",
    "    inputs = [word_to_ix[word] for word in tokenize(text)]\n",
    "    for t in range(len(inputs)):\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[inputs[t]] = 1\n",
    "        hprev = np.tanh(np.dot(Wxh, x) + np.dot(Whh, hprev) + Bh)\n",
    "\n",
    "    y = np.dot(Why, hprev) + By\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    \n",
    "    return ix_to_word[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 145.128377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 100, loss: 146.523265\n",
      "iter 200, loss: 143.630763\n",
      "iter 300, loss: 140.125037\n",
      "iter 400, loss: 137.665253\n",
      "iter 500, loss: 133.514947\n",
      "iter 600, loss: 128.881529\n",
      "iter 700, loss: 123.922085\n",
      "iter 800, loss: 118.686506\n",
      "iter 900, loss: 113.248832\n",
      "iter 1000, loss: 107.811971\n",
      "iter 1100, loss: 102.255217\n",
      "iter 1200, loss: 96.679482\n",
      "iter 1300, loss: 91.298546\n",
      "iter 1400, loss: 85.892549\n",
      "iter 1500, loss: 80.627141\n",
      "iter 1600, loss: 75.589277\n",
      "iter 1700, loss: 70.694775\n",
      "iter 1800, loss: 66.029997\n",
      "iter 1900, loss: 61.586191\n",
      "iter 2000, loss: 57.210330\n",
      "iter 2100, loss: 53.089178\n",
      "iter 2200, loss: 49.203173\n",
      "iter 2300, loss: 45.481933\n",
      "iter 2400, loss: 42.079740\n",
      "iter 2500, loss: 38.872600\n",
      "iter 2600, loss: 35.801845\n",
      "iter 2700, loss: 32.947669\n",
      "iter 2800, loss: 30.295755\n",
      "iter 2900, loss: 27.956856\n",
      "iter 3000, loss: 25.672378\n",
      "iter 3100, loss: 23.550706\n",
      "iter 3200, loss: 21.589493\n",
      "iter 3300, loss: 19.786788\n",
      "iter 3400, loss: 18.128322\n",
      "iter 3500, loss: 16.607625\n",
      "iter 3600, loss: 15.217574\n",
      "iter 3700, loss: 13.943614\n",
      "iter 3800, loss: 12.779202\n",
      "iter 3900, loss: 11.716649\n",
      "iter 4000, loss: 10.744514\n",
      "iter 4100, loss: 9.857123\n",
      "iter 4200, loss: 9.047998\n",
      "iter 4300, loss: 8.307570\n",
      "iter 4400, loss: 7.632551\n",
      "iter 4500, loss: 7.017053\n",
      "iter 4600, loss: 6.453418\n",
      "iter 4700, loss: 5.939610\n",
      "iter 4800, loss: 5.470801\n",
      "iter 4900, loss: 5.041515\n",
      "iter 5000, loss: 4.649970\n",
      "iter 5100, loss: 4.292547\n",
      "iter 5200, loss: 3.965035\n",
      "iter 5300, loss: 3.666574\n",
      "iter 5400, loss: 3.393434\n",
      "iter 5500, loss: 3.143319\n",
      "iter 5600, loss: 2.915235\n",
      "iter 5700, loss: 2.706117\n",
      "iter 5800, loss: 2.514559\n",
      "iter 5900, loss: 2.339760\n",
      "iter 6000, loss: 2.179161\n",
      "iter 6100, loss: 2.032043\n",
      "iter 6200, loss: 1.897582\n",
      "iter 6300, loss: 1.773914\n",
      "iter 6400, loss: 1.660592\n",
      "iter 6500, loss: 1.556871\n",
      "iter 6600, loss: 1.461118\n",
      "iter 6700, loss: 1.373474\n",
      "iter 6800, loss: 1.293101\n",
      "iter 6900, loss: 1.218655\n",
      "iter 7000, loss: 1.150486\n",
      "iter 7100, loss: 1.087799\n",
      "iter 7200, loss: 1.029699\n",
      "iter 7300, loss: 0.976341\n",
      "iter 7400, loss: 0.927173\n",
      "iter 7500, loss: 0.881445\n",
      "iter 7600, loss: 0.839494\n",
      "iter 7700, loss: 0.800580\n",
      "iter 7800, loss: 0.764386\n",
      "iter 7900, loss: 0.731135\n",
      "iter 8000, loss: 0.700112\n",
      "iter 8100, loss: 0.671231\n",
      "iter 8200, loss: 0.644660\n",
      "iter 8300, loss: 0.619720\n",
      "iter 8400, loss: 0.596513\n",
      "iter 8500, loss: 0.575035\n",
      "iter 8600, loss: 0.554827\n",
      "iter 8700, loss: 0.536017\n",
      "iter 8800, loss: 0.518527\n",
      "iter 8900, loss: 0.501907\n",
      "iter 9000, loss: 0.486505\n",
      "iter 9100, loss: 0.472115\n",
      "iter 9200, loss: 0.458331\n",
      "iter 9300, loss: 0.445574\n",
      "iter 9400, loss: 0.433561\n",
      "iter 9500, loss: 0.422082\n",
      "iter 9600, loss: 0.411365\n",
      "iter 9700, loss: 0.401247\n",
      "iter 9800, loss: 0.391514\n",
      "iter 9900, loss: 0.382473\n"
     ]
    }
   ],
   "source": [
    "for n in range(num_iterations):\n",
    "    if p + seq_length + 1 >= len(words) or n == 0:\n",
    "        hprev = np.zeros((hidden_size, 1))\n",
    "        p = 0\n",
    "    inputs = [word_to_ix[word] for word in words[p:p + seq_length]]\n",
    "    targets = [word_to_ix[word] for word in words[p + 1:p + seq_length + 1]]\n",
    "\n",
    "    loss, dWxh, dWhh, dWhy, dBh, dBy, hprev = rnn(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "    if n % 100 == 0:\n",
    "        print('iter %d, loss: %f' % (n, smooth_loss))\n",
    "\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, Bh, By],\n",
    "                                  [dWxh, dWhh, dWhy, dBh, dBy],\n",
    "                                  [mWxh, mWhh, mWhy, mBh, mBy]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)\n",
    "\n",
    "    p += seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predictions(text, next_word, hprev):\n",
    "\n",
    "    predicted_word = predict_next(text, hprev)\n",
    "    print(\"Seed text: '{}'\".format(text))\n",
    "    print(\"Actual next word: '{}'\".format(next_word))\n",
    "    print(\"Predicted next word: '{}'\".format(predicted_word))\n",
    "    if predicted_word == next_word:\n",
    "        print(\"Prediction is correct!\")\n",
    "    else:\n",
    "        print(\"Prediction is incorrect.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing the RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed text: 'the script the set designer builds the scenery the costume designer creates outfits the makeup artist enhances appearances the cinematographer captures the shots the editor'\n",
      "Actual next word: 'assembles'\n",
      "Predicted next word: 'assembles'\n",
      "Prediction is correct!\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(100)  \n",
    "start_idx = np.random.randint(0, len(words) - seq_length - 1)\n",
    "seed_text = ' '.join(words[start_idx:start_idx + seq_length])\n",
    "next_word = words[start_idx + seq_length] \n",
    "\n",
    "hprev = np.zeros((hidden_size, 1))\n",
    "compare_predictions(seed_text, next_word, hprev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
